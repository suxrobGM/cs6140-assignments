{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import the required packages and load the lending club dataset from CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the parent directory to the Python path to import the core module\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "from core import get_data_path\n",
    "\n",
    "csv_file_path = get_data_path(\"assignment2/lc_14to16.csv\")\n",
    "data = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summary (Exploratory Data Analysis)\n",
    "Perform EDA to analyze the differences between the two datasets (before and after the 2015 controversy).\n",
    "\n",
    "Histograms used to compare the distribution of the loan amount, annual income, and debt-to-income ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert issue date to datetime to filter by time period\n",
    "data[\"issue_d\"] = pd.to_datetime(data[\"issue_d\"])\n",
    "\n",
    "# Define the controversy date\n",
    "controversy_date = pd.to_datetime(\"2015-01-01\")\n",
    "\n",
    "# Create \"Period\" column based on the controversy date\n",
    "# Create a new column to indicate the period, values are \"Before 2015\" and \"After 2015\"\n",
    "data[\"Period\"] = np.where(data[\"issue_d\"] < controversy_date, \"Before 2015\", \"After 2015\")\n",
    "\n",
    "# Print basic information for both periods\n",
    "print(\"Total columns in original dataset: \", data.shape[1])\n",
    "print(\"Total rows in original dataset: \", data.shape[0])\n",
    "print(\"Total rows in dataset before 2015:\", data[data[\"Period\"] == \"Before 2015\"].shape[0])\n",
    "print(\"Total rows in dataset after 2015:\", data[data[\"Period\"] == \"After 2015\"].shape[0])\n",
    "\n",
    "# Print the first few rows of the dataset\n",
    "print(\"First few rows of the dataset: \")\n",
    "display(data.head())\n",
    "\n",
    "# Print the summary statistics of the dataset\n",
    "print(\"Summary Statistics: \")\n",
    "display(data.describe())\n",
    "\n",
    "# Calculate missing values in the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values count:\", )\n",
    "display(missing_values[missing_values > 0])\n",
    "\n",
    "# Plot distributions for loan amount\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data, x=\"loan_amnt\", hue=\"Period\", kde=True, bins=30)\n",
    "plt.title(\"Loan Amount Distribution Before and After Controversy\")\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot interest rate distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data, x=\"int_rate\", hue=\"Period\", kde=True, bins=30)\n",
    "plt.title(\"Interest Rate Distribution Before and After Controversy\")\n",
    "plt.xlabel(\"Interest Rate (%)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Plot grade distribution\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(data=data, x=\"grade\", hue=\"Period\", order=sorted(data[\"grade\"].unique()))\n",
    "plt.title(\"Loan Grade Distribution Before and After Controversy\")\n",
    "plt.xlabel(\"Grade\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze Differences Between Periods\n",
    "# Loan Purpose\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=data, y=\"purpose\", hue=\"Period\", order=data[\"purpose\"].value_counts().index)\n",
    "plt.title(\"Loan Purpose Distribution Before and After Controversy\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Purpose\")\n",
    "plt.show()\n",
    "\n",
    "# Employment Length\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=data, x=\"emp_length\", hue=\"Period\", order=sorted(data[\"emp_length\"].dropna().unique()))\n",
    "plt.title(\"Employment Length Before and After Controversy\")\n",
    "plt.xlabel(\"Employment Length (Years)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Home Ownership\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(data=data, x=\"home_ownership\", hue=\"Period\")\n",
    "plt.title(\"Home Ownership Status Before and After Controversy\")\n",
    "plt.xlabel(\"Home Ownership\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "# Select numerical features\n",
    "numerical_features = data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = data[numerical_features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "### a. Standardize the numerical features and encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = data.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "categorical_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].median())\n",
    "\n",
    "# Fill categorical missing values with mode\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "# List of numerical features to standardize\n",
    "num_features = [\"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"open_acc\", \"revol_bal\", \"total_acc\"]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "data[num_features] = scaler.fit_transform(data[num_features])\n",
    "\n",
    "# Encode \"grade\"\n",
    "label_enc = LabelEncoder()\n",
    "data[\"grade\"] = label_enc.fit_transform(data[\"grade\"])\n",
    "\n",
    "# Nominal categorical features to encode\n",
    "nominal_features = [\"home_ownership\", \"verification_status\", \"purpose\", \"addr_state\", \"application_type\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "data = pd.get_dummies(data, columns=nominal_features, drop_first=True)\n",
    "\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Identify and remove up to 1% of rows as outliers based on standardized `dti`, `annualincome`, and `delinq_2yrs` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Features to check for outliers\n",
    "outlier_features = [\"dti\", \"annual_inc\", \"delinq_2yrs\"]\n",
    "\n",
    "# Standardize these features (if not already done)\n",
    "data[outlier_features] = scaler.fit_transform(data[outlier_features])\n",
    "\n",
    "# Sum the absolute standardized scores\n",
    "data[\"outlier_score\"] = data[outlier_features].abs().sum(axis=1)\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = data[\"outlier_score\"].quantile(0.25)\n",
    "Q3 = data[\"outlier_score\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = data[(data[\"outlier_score\"] < lower_bound) | (data[\"outlier_score\"] > upper_bound)]\n",
    "\n",
    "# Determine the number of outliers to remove (up to 1% of total data)\n",
    "max_outliers = int(0.01 * len(data))\n",
    "outliers_to_remove = outliers.head(max_outliers)\n",
    "\n",
    "# Remove outliers\n",
    "data_cleaned = data.drop(outliers_to_remove.index).reset_index(drop=True)\n",
    "\n",
    "# Calculate the percentage of data retained\n",
    "retained_data_percent = (data_cleaned.shape[0] / data.shape[0]) * 100\n",
    "\n",
    "# Check the shape of the cleaned data\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Cleaned Data Shape:\", data_cleaned.shape)\n",
    "print(\"Number of Outliers Removed:\", outliers_to_remove.shape[0])\n",
    "print(\"Percentage of Data Retained: {:.2f}%\".format(retained_data_percent))\n",
    "\n",
    "# Remove temporary columns\n",
    "data_cleaned = data_cleaned.drop(columns=[\"outlier_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Task\n",
    "### Define response variable and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data to avoid modifying the original DataFrame\n",
    "data_ml = data_cleaned.copy()\n",
    "\n",
    "print(\"Shape of data_ml:\", data_ml.shape)\n",
    "\n",
    "# Map \"grade\" to High-Low response\n",
    "def map_high_low(grade):\n",
    "    if grade in [\"A\", \"B\"]:\n",
    "        return \"High\"\n",
    "    elif grade in [\"D\", \"E\", \"F\", \"G\"]:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return np.nan  # Exclude \"C\"\n",
    "\n",
    "# Map \"grade\" to High-Medium-Low response\n",
    "def map_high_med_low(grade):\n",
    "    if grade in [\"A\", \"B\"]:\n",
    "        return \"High\"\n",
    "    elif grade == \"C\":\n",
    "        return \"Medium\"\n",
    "    elif grade in [\"D\", \"E\", \"F\", \"G\"]:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Remove ' months' and convert to integer\n",
    "data_ml[\"term\"] = data_ml[\"term\"].str.strip().str.replace(\" months\", \"\").astype(int)\n",
    "\n",
    "# Apply the mapping functions\n",
    "data_ml[\"High_Low\"] = data_ml[\"grade\"].map(map_high_low)\n",
    "data_ml[\"High_Med_Low\"] = data_ml[\"grade\"].map(map_high_med_low)\n",
    "\n",
    "# Drop rows with NaN in \"High_Low\" (i.e., where grade is \"C\")\n",
    "data_high_low = data_ml.dropna(subset=[\"High_Low\"]).reset_index(drop=True)\n",
    "\n",
    "# Drop rows with NaN in \"High_Med_Low\" (i.e., where grade is not A-G)\n",
    "data_high_med_low = data_ml.dropna(subset=[\"High_Med_Low\"]).reset_index(drop=True)\n",
    "\n",
    "# Verify that data_high_low is not empty\n",
    "print(\"Number of samples in data_high_low:\", data_high_low.shape[0])\n",
    "print(\"Number of samples in data_high_med_low:\", data_high_med_low.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Split the Data into Train-Validate-Test Sets\n",
    "I split the data into:\n",
    "\n",
    "- Training Set: 70%\n",
    "- Validation Set: 15%\n",
    "- Test Set: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target for High-Low response\n",
    "X = data_high_low.drop([\"High_Low\", \"High_Med_Low\"], axis=1)\n",
    "y = data_high_low[\"High_Low\"]\n",
    "\n",
    "# Verify that X and y have the same number of samples\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# First split: Train and temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Second split: Validation and Test from temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "object_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Categorical columns:\", object_cols)\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "\n",
    "for col in object_cols:\n",
    "    data_ml[col] = label_enc.fit_transform(data_ml[col])\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Build a logistic model to accurately predict the High-Low response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(\"Data types in X_train:\")\n",
    "display(X_train.dtypes)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "X = data_high_low.drop([\"High_Low\", \"High_Med_Low\"], axis=1)\n",
    "y = data_high_low[\"High_Low\"]\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "data_high_low[\"grade\"] = label_enc.fit_transform(data_high_low[\"grade\"])\n",
    "data_high_low[\"sub_grade\"] = label_enc.fit_transform(data_high_low[\"sub_grade\"])\n",
    "display(data_high_low.head())\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred, pos_label=\"High\")\n",
    "recall = recall_score(y_val, y_val_pred, pos_label=\"High\")\n",
    "\n",
    "print(\"Logistic Regression (High-Low) Validation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
