{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Import the required packages and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Add the parent directory to the Python path to import the core module\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the csv file from dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suxro\\AppData\\Local\\Temp\\ipykernel_34508\\3583458121.py:5: DtypeWarning: Columns (19,59,129,130,131,134,135,136,139) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lc = pd.read_csv(csv_file_path)\n"
     ]
    }
   ],
   "source": [
    "from core import get_data_path\n",
    "\n",
    "csv_file_path = get_data_path(\"assignment2/lc_14to16.csv\")\n",
    "data = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data cleaning examples \n",
    "# ----------------------------\n",
    "data.iloc[:,0:20].head()\n",
    "\n",
    "\n",
    "# Example Task 1: Combine employment length into 3 categories:\n",
    "# #  \"< 1 year\", \"1 year\" = 0-1 years\n",
    "# # \"2 years, 3 years, 4 years =  2-4 years\n",
    "# # everything else =  5+ years\n",
    "\n",
    "data[\"emp_length\"] = data[\"emp_length\"].replace([\"< 1 year\", \"1 year\"], \"0-1 years\")\n",
    "data[\"emp_length\"] = data[\"emp_length\"].replace([\"2 years\", \"3 years\", \"4 years\", \"5 years\"], \"2-5 years\")\n",
    "data[\"emp_length\"] = data[\"emp_length\"].replace([ \"6 years\", \"7 years\", \"8 years\", \"9 years\", \"10+ years\"], \"5+ years\")\n",
    "\n",
    "# lc[\"emp_length\"].value_counts()\n",
    "\n",
    "# Example  Task 2: remove \"ANY\" category data for home_ownership\n",
    "data = data[data.home_ownership != \"ANY\"]\n",
    "\n",
    "data[\"home_ownership\"].value_counts()\n",
    "\n",
    "\n",
    "# Example Task 3: Subset data with only the following columns: \"loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"grade\", \"emp_length\", \"home_ownership\"\n",
    "\n",
    "data = data[[\"loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\", \"grade\", \"emp_length\", \"home_ownership\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical variables: \"loan_amnt\", \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"\n",
    "\n",
    "start = datetime.now()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "to_scale = [\"loan_amnt\", \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"]\n",
    "data[to_scale] = scaler.fit_transform(data[to_scale])\n",
    "stop = datetime.now()\n",
    "print(stop - start)\n",
    "\n",
    "data.head()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Report of missing values\n",
    "# ---------------------------------------------------------------------------------\n",
    "data.isnull().sum()\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# Eliminate rows with missing values\n",
    "# One must be careful with this step, as it can lead to a huge loss of information\n",
    "# Later we will see how to impute missing values using multidimesional imputation\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "data = data.dropna()\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------- \n",
    "# Convert grade to a tertiary variable: A,B = High, DEFG = Low,C = Medium\n",
    "# Adding a new column to the dataframe called \"grade_tertiary\"\n",
    "# ---------------------------------------------------------------- \n",
    "start = datetime.now()\n",
    "\n",
    "data[\"grade_tertiary\"] = data[\"grade\"].replace([\"A\", \"B\"], \"3.High\").replace([\"C\"], \"2.Medium\").replace([\"D\", \"E\", \"F\", \"G\"], \"1.Low\")\n",
    "data[\"grade_tertiary\"].value_counts()\n",
    "\n",
    "\n",
    "# filter out Medium for the grade_tertiary column \n",
    "lc2 = data[data[\"grade_tertiary\"] != \"2.Medium\"]\n",
    "\n",
    "stop = datetime.now()\n",
    "print(stop - start)\n",
    "\n",
    "print(data[\"grade_tertiary\"].value_counts())\n",
    "print(lc2[\"grade_tertiary\"].value_counts())\n",
    "\n",
    "lc2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical features: \"emp_length\", \"home_ownership\"\n",
    "\n",
    "start = datetime.now()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "to_encode = [\"emp_length\", \"home_ownership\"]\n",
    "lc_encoded = pd.get_dummies(lc2, columns = to_encode)\n",
    "\n",
    "stop = datetime.now()\n",
    "print(stop - start)\n",
    "\n",
    "lc_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression on grade_tertiary as the target variable and the rest (exclusing grade) as the predictors\n",
    "\n",
    "X = lc_encoded.drop(columns = [\"grade_tertiary\", \"grade\"])\n",
    "y = lc_encoded[\"grade_tertiary\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "logit = LogisticRegression(max_iter = 1000)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Classification report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print a labeled confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"1.Low\", \"3.High\"], yticklabels=[\"1.Low\", \"3.High\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(logit.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Add predicted probability to the dataframe\n",
    "# ---------------------------------------------------\n",
    "\n",
    "lc_encoded[\"predicted\"] = logit.predict(X)\n",
    "lc_encoded[\"predicted_prob\"] = logit.predict_proba(X)[:,1]\n",
    "lc_encoded.head()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ROC curve\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logit.predict_proba(X_test)[:,1], pos_label=\"3.High\")\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat the above by changing the threshold to 0.6\n",
    "\n",
    "y_pred_06 = (logit.predict_proba(X_test)[:,1] > 0.6).astype(int)\n",
    "\n",
    "# # Convert predicted labels back to original label types\n",
    "y_pred_06 = np.where(y_pred_06 == 1, \"3.High\", \"1.Low\")\n",
    "\n",
    "print(classification_report(y_test, y_pred_06))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_06)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"1.Low\", \"3.High\"], yticklabels=[\"1.Low\", \"3.High\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ROC curve\n",
    "# ---------------------------------------------------\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logit.predict_proba(X_test)[:,1], pos_label=\"3.High\")\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Losgistic regression sklearn page:https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Hyperparameter tuning for logistic regression\n",
    "# the most important hyperparameter for logistic regression is C, which is the inverse of the regularization strength   \n",
    "# smaller values of C specify stronger regularization\n",
    "# Regularization is a technique used to prevent overfitting by penalizing large coefficients\n",
    "# The default value of C is 1\n",
    "\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logit = LogisticRegression(max_iter = 1000)\n",
    "logit_cv = GridSearchCV(logit, param_grid, cv = 5)\n",
    "logit_cv.fit(X_train, y_train)\n",
    "\n",
    "print(logit_cv.best_params_)\n",
    "print(logit_cv.best_score_)\n",
    "print(logit_cv.best_estimator_)\n",
    "print(logit_cv.best_index_)\n",
    "print(logit_cv.best_estimator_.C)\n",
    "\n",
    "\n",
    "\n",
    "# create a df with the results of the grid search\n",
    "# This is done to see the results of the grid search\n",
    "results = pd.DataFrame(logit_cv.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Replicating the above steps for the KNN classifier\n",
    "# ---------------------------------------------------\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"1.Low\", \"3.High\"], yticklabels=[\"1.Low\", \"3.High\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# hyperparameter tuning for KNN\n",
    "# ---------------------------------------------------\n",
    "\n",
    "param_grid = {\"n_neighbors\": [5, 7, 9, 11 ]}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "print(knn_cv.best_params_)\n",
    "print(knn_cv.best_score_)\n",
    "print(knn_cv.best_estimator_)\n",
    "\n",
    "results = pd.DataFrame(knn_cv.cv_results_)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# And now Replicating for the SGD classifier\n",
    "# ---------------------------------------------------\n",
    "sgd = SGDClassifier(loss=\"log\")\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[\"1.Low\", \"3.High\"], yticklabels=[\"1.Low\", \"3.High\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# hyperparameter tuning for SGD\n",
    "# ---------------------------------------------------\n",
    "\n",
    "param_grid = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "sgd = SGDClassifier()\n",
    "sgd_cv = GridSearchCV(sgd, param_grid, cv = 5)\n",
    "\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "\n",
    "print(sgd_cv.best_params_)\n",
    "print(sgd_cv.best_score_)\n",
    "print(sgd_cv.best_estimator_)\n",
    "print(sgd_cv.best_index_)\n",
    "print(sgd_cv.best_estimator_.alpha)\n",
    "\n",
    "results = pd.DataFrame(sgd_cv.cv_results_)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics on the following columns: loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"\n",
    "lc2[[\"loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate rows with missing values in the following columns: loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"\n",
    "lc3 = lc2.dropna(subset=[\"loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"])\n",
    "lc3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression on grade_tertiary as the target variable and some numerical  as predictors\n",
    "# Use the following predictors: \"loan_amnt\", \"int_rate\", \"annual_inc\", \"dti\", \"fico_range_low\", \"fico_range_high\", \"revol_bal\", \"revol_util\", \"total_pymnt\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_pymnt_amnt\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"\n",
    "\n",
    "X = lc3[[\"loan_amnt\",  \"annual_inc\", \"dti\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"]]\n",
    "\n",
    "y = lc3[\"grade_tertiary\"]\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
